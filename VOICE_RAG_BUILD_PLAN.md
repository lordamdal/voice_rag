# Voice RAG Assistant â€” Build Plan

## Overview

A local voice-based RAG assistant running on a Dell laptop with 4GB VRAM (RTX 500 Ada) + 16GB RAM. The system uses a modular pipeline: **Whisper (STT) â†’ RAG retrieval â†’ Ollama LLM â†’ Piper (TTS)**, with a React frontend for the user interface and a Python FastAPI backend orchestrating everything.

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   REACT FRONTEND                        â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Mic Input â”‚  â”‚ Chat Display â”‚  â”‚ Document Upload   â”‚  â”‚
â”‚  â”‚ (WebAudio)â”‚  â”‚ (transcript) â”‚  â”‚ (for RAG ingest)  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚        â”‚               â”‚                   â”‚             â”‚
â”‚        â”‚  WebSocket     â”‚  SSE/WS           â”‚  REST       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚               â”‚                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   PYTHON BACKEND (FastAPI)                â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  /ws/voice   â”‚  â”‚  /api/chat   â”‚  â”‚ /api/documents  â”‚  â”‚
â”‚  â”‚  WebSocket   â”‚  â”‚  text route  â”‚  â”‚ upload & ingest â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                 â”‚                   â”‚           â”‚
â”‚         â–¼                 â–¼                   â–¼           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                  PIPELINE ORCHESTRATOR               â”‚  â”‚
â”‚  â”‚                                                     â”‚  â”‚
â”‚  â”‚  1. STT (Whisper tiny)          â€” GPU ~150MB        â”‚  â”‚
â”‚  â”‚  2. Embedding (MiniLM-L6-v2)   â€” CPU ~90MB         â”‚  â”‚
â”‚  â”‚  3. Retrieval (ChromaDB)        â€” CPU               â”‚  â”‚
â”‚  â”‚  4. LLM (Ollama qwen3:1.7b)    â€” GPU ~1.2GB        â”‚  â”‚
â”‚  â”‚  5. TTS (Piper ONNX)           â€” CPU ~60MB         â”‚  â”‚
â”‚  â”‚  6. Conversation Memory         â€” ChromaDB          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Tech Stack

| Layer         | Technology                        | Notes                                  |
| ------------- | --------------------------------- | -------------------------------------- |
| Frontend      | React + Vite + Tailwind           | WebAudio API for mic, WebSocket for streaming |
| Backend       | Python 3.11+ / FastAPI            | Async, WebSocket support               |
| STT           | `faster-whisper` (tiny model)     | CTranslate2 optimized, GPU             |
| LLM           | Ollama (`qwen3:1.7b`)            | Already installed, REST API on :11434  |
| TTS           | `piper-tts`                       | ONNX runtime, CPU, offline             |
| Embeddings    | `sentence-transformers` MiniLM    | CPU inference                          |
| Vector DB     | ChromaDB                          | Local persistent storage               |
| Audio         | WebSocket binary frames           | Raw PCM or WAV chunks                  |

---

## Project Structure

```
voice-rag-assistant/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py                  # FastAPI app entry point
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ config.py                # Model paths, Ollama URL, settings
â”‚   â”œâ”€â”€ pipeline/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ stt.py               # Whisper transcription service
â”‚   â”‚   â”œâ”€â”€ llm.py               # Ollama client wrapper
â”‚   â”‚   â”œâ”€â”€ tts.py               # Piper TTS service
â”‚   â”‚   â”œâ”€â”€ rag.py               # ChromaDB + embedding + retrieval
â”‚   â”‚   â””â”€â”€ orchestrator.py      # Wires STT â†’ RAG â†’ LLM â†’ TTS
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ whisper/              # faster-whisper tiny model cache
â”‚   â”‚   â””â”€â”€ piper/                # Piper voice model (.onnx + .json)
â”‚   â””â”€â”€ data/
â”‚       â””â”€â”€ chroma_db/            # ChromaDB persistent storage
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ vite.config.ts
â”‚   â”œâ”€â”€ tailwind.config.js
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.tsx              # Main app layout
â”‚   â”‚   â”œâ”€â”€ main.tsx
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ VoiceButton.tsx   # Push-to-talk / toggle mic
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatPanel.tsx     # Scrollable transcript
â”‚   â”‚   â”‚   â”œâ”€â”€ WaveformViz.tsx   # Audio visualizer
â”‚   â”‚   â”‚   â”œâ”€â”€ StatusBar.tsx     # Pipeline stage indicator
â”‚   â”‚   â”‚   â”œâ”€â”€ DocumentUpload.tsx# Drag & drop doc upload
â”‚   â”‚   â”‚   â””â”€â”€ SettingsPanel.tsx # Model select, voice select
â”‚   â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”‚   â”œâ”€â”€ useAudioRecorder.ts   # WebAudio mic capture
â”‚   â”‚   â”‚   â”œâ”€â”€ useWebSocket.ts       # WS connection manager
â”‚   â”‚   â”‚   â””â”€â”€ useAudioPlayer.ts     # Play TTS audio response
â”‚   â”‚   â”œâ”€â”€ stores/
â”‚   â”‚   â”‚   â””â”€â”€ chatStore.ts     # Zustand store for messages
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â””â”€â”€ audioUtils.ts    # PCM conversion helpers
â”‚
â”œâ”€â”€ docker-compose.yml           # Optional: containerized setup
â””â”€â”€ README.md
```

---

## Build Phases

### Phase 1 â€” Backend Core Pipeline (Priority: HIGH)

**Goal:** Working STT â†’ LLM â†’ TTS loop via REST endpoint.

#### Steps:
1. **Setup FastAPI project** with CORS, static file serving
2. **STT service** (`stt.py`)
   - Load `faster-whisper` tiny model on GPU
   - Accept WAV/PCM bytes â†’ return transcription text
   - Use `compute_type="int8"` for VRAM savings
3. **LLM service** (`llm.py`)
   - HTTP client to Ollama REST API (`http://localhost:11434/api/chat`)
   - Streaming response support (SSE)
   - System prompt with conversation context window
4. **TTS service** (`tts.py`)
   - Load Piper voice model (en_US-amy-medium recommended)
   - Accept text â†’ return WAV audio bytes
   - CPU-only inference via ONNX runtime
5. **Orchestrator** (`orchestrator.py`)
   - Wire: audio bytes â†’ STT â†’ LLM â†’ TTS â†’ audio bytes
   - Return both text transcript AND audio response
6. **REST endpoint** (`POST /api/voice`)
   - Accepts audio file upload
   - Returns JSON `{ transcript, response_text, audio_url }`

#### Test checkpoint:
```bash
curl -X POST http://localhost:8000/api/voice \
  -F "audio=@test_recording.wav"
# Should return transcript + response text + audio file URL
```

---

### Phase 2 â€” RAG Layer (Priority: HIGH)

**Goal:** Document ingestion + context-aware responses.

#### Steps:
1. **RAG service** (`rag.py`)
   - Initialize ChromaDB persistent client
   - Load `all-MiniLM-L6-v2` sentence-transformer on CPU
   - `ingest_document(file)` â€” chunk text (500 chars, 50 overlap) â†’ embed â†’ store
   - `retrieve(query, k=3)` â€” embed query â†’ similarity search â†’ return chunks
2. **Document upload endpoint** (`POST /api/documents`)
   - Accept PDF, TXT, MD files
   - Parse text (use `pymupdf` for PDFs)
   - Chunk and ingest into ChromaDB
   - Return `{ doc_id, chunk_count, status }`
3. **Integrate RAG into orchestrator**
   - After STT: embed user query â†’ retrieve relevant chunks
   - Inject chunks into LLM system prompt:
     ```
     Context from documents:
     ---
     {chunk_1}
     {chunk_2}
     ---
     Answer the user's question using the context above.
     User: {transcribed_text}
     ```
4. **Conversation memory**
   - Store each exchange (user + assistant) as embeddings in a separate ChromaDB collection
   - On each turn, also retrieve top-2 relevant past exchanges
   - This gives "long context" without actually using a big context window

#### Test checkpoint:
- Upload a PDF about a specific topic
- Ask a voice question about it
- Verify the response references document content

---

### Phase 3 â€” WebSocket Voice Streaming (Priority: MEDIUM)

**Goal:** Real-time voice interaction instead of upload-and-wait.

#### Steps:
1. **WebSocket endpoint** (`/ws/voice`)
   - Accept binary audio frames from frontend mic
   - Buffer frames until silence detection (VAD)
   - On silence: run full pipeline
   - Stream back: `{ type: "transcript", text }` â†’ `{ type: "response", text }` â†’ `{ type: "audio", data: base64 }`
2. **Voice Activity Detection**
   - Use `webrtcvad` or `silero-vad` for endpoint detection
   - Configurable silence threshold (300-500ms)
3. **Pipeline status events**
   - Send status updates over WS: `listening` â†’ `transcribing` â†’ `thinking` â†’ `speaking`
   - Frontend shows current stage to user

---

### Phase 4 â€” React Frontend (Priority: HIGH, parallel with Phase 1)

**Goal:** Clean, functional UI for voice interaction.

#### Components:

1. **VoiceButton.tsx**
   - Large central mic button (push-to-talk or toggle)
   - Visual states: idle / listening (pulsing) / processing (spinning) / speaking (waveform)
   - Uses `useAudioRecorder` hook for WebAudio capture
   - Sends audio via WebSocket or as POST to `/api/voice`

2. **ChatPanel.tsx**
   - Scrollable message list showing:
     - User messages (transcribed speech, right-aligned)
     - Assistant messages (LLM response, left-aligned)
     - Timestamps and latency badges (e.g., "STT: 180ms | LLM: 1.2s | TTS: 90ms")
   - Auto-scroll to latest message

3. **WaveformViz.tsx**
   - Real-time audio waveform during recording (AnalyserNode)
   - Playback waveform during TTS response
   - Subtle, ambient â€” not distracting

4. **StatusBar.tsx**
   - Pipeline stage indicator: ðŸŽ¤ â†’ ðŸ“ â†’ ðŸ§  â†’ ðŸ”Š
   - Shows which step is currently active
   - Displays VRAM/RAM usage if available

5. **DocumentUpload.tsx**
   - Drag & drop zone for PDFs/TXT
   - Shows list of ingested documents with chunk counts
   - Delete document button (removes from ChromaDB)

6. **SettingsPanel.tsx**
   - Collapsible sidebar
   - Model selector (dropdown of Ollama models)
   - Voice selector (Piper voices)
   - RAG toggle on/off
   - Temperature / max tokens sliders

#### Frontend Hooks:

- **useAudioRecorder** â€” start/stop mic, capture PCM 16-bit 16kHz, return blob
- **useWebSocket** â€” connect to `/ws/voice`, send binary, receive JSON + audio
- **useAudioPlayer** â€” queue and play WAV/PCM responses via AudioContext

---

### Phase 5 â€” Polish & Optimization (Priority: LOW)

1. **Latency dashboard** â€” measure and display each pipeline stage timing
2. **Keyboard shortcut** â€” spacebar for push-to-talk
3. **Dark/light theme** toggle
4. **Export conversation** as markdown
5. **Model hot-swap** â€” change Ollama model without restart
6. **Chunking strategy tuning** â€” experiment with chunk sizes for RAG quality

---

## Dependencies

### Backend (`requirements.txt`)
```
fastapi>=0.104.0
uvicorn[standard]>=0.24.0
python-multipart>=0.0.6
websockets>=12.0
faster-whisper>=1.0.0
piper-tts>=1.2.0
chromadb>=0.4.22
sentence-transformers>=2.3.0
pymupdf>=1.23.0
httpx>=0.25.0
webrtcvad>=2.0.10
numpy>=1.24.0
```

### Frontend (`package.json` key deps)
```json
{
  "dependencies": {
    "react": "^18.3.0",
    "react-dom": "^18.3.0",
    "zustand": "^4.5.0"
  },
  "devDependencies": {
    "vite": "^5.4.0",
    "tailwindcss": "^3.4.0",
    "typescript": "^5.5.0",
    "@types/react": "^18.3.0"
  }
}
```

---

## API Endpoints Summary

| Method | Path              | Description                     | Input              | Output                              |
| ------ | ----------------- | ------------------------------- | ------------------ | ----------------------------------- |
| POST   | `/api/voice`      | Full voice pipeline             | `audio` file       | `{ transcript, response, audio_url }` |
| POST   | `/api/chat`       | Text-only chat (no voice)       | `{ message }`      | `{ response }` (SSE stream)         |
| POST   | `/api/documents`  | Upload & ingest document        | `file` (PDF/TXT)   | `{ doc_id, chunks, status }`        |
| GET    | `/api/documents`  | List ingested documents         | â€”                  | `[{ doc_id, name, chunks }]`        |
| DELETE | `/api/documents/{id}` | Remove document from RAG    | â€”                  | `{ status }`                        |
| GET    | `/api/models`     | List available Ollama models    | â€”                  | `[{ name, size, quantization }]`    |
| GET    | `/api/status`     | System health + resource usage  | â€”                  | `{ gpu_mem, ram, models_loaded }`   |
| WS     | `/ws/voice`       | Real-time voice streaming       | binary audio frames | JSON events + binary audio          |

---

## Environment & Config

```env
# .env
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen3:1.7b
WHISPER_MODEL=tiny
WHISPER_DEVICE=cuda
WHISPER_COMPUTE_TYPE=int8
PIPER_VOICE=en_US-amy-medium
CHROMA_PERSIST_DIR=./data/chroma_db
EMBEDDING_MODEL=all-MiniLM-L6-v2
RAG_CHUNK_SIZE=500
RAG_CHUNK_OVERLAP=50
RAG_TOP_K=3
```

---

## Build Order (Recommended)

```
Week 1:  Phase 1 (backend pipeline) + Phase 4 scaffolding (React app shell)
Week 2:  Phase 2 (RAG) + Phase 4 components (chat, voice button, upload)
Week 3:  Phase 3 (WebSocket streaming) + Phase 4 hooks (real-time audio)
Week 4:  Phase 5 (polish, latency tuning, UI refinement)
```

For a fast test/demo, **Phase 1 + Phase 4 (basic)** can be done in a single session â€” that gives you a working voice-in â†’ text-out â†’ voice-out loop with a simple React UI.
